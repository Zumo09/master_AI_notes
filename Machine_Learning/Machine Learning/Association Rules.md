# Association Rules

# The Market Basket

Given a set of commercial transactions, find rules that will predict the occurrence of an item based on the occurrences of other items in the transaction

![Association%20Rules%2036d8e18230e44e5e8c95178312fd2fd9/Untitled.png](Association%20Rules%2036d8e18230e44e5e8c95178312fd2fd9/Untitled.png)

- {Diaper} → {Beer}
- {Bread, Milk} → {Coke, Eggs}

Implication means co-occurrence, not causality!

The implication in Association Rules is different from that of logic: it can be true *with some level of truth*

## Definitions

- **Itemset** : A collection of one or more items
    - {Milk, Bread, Diaper}
- **k-itemset** : An itemset that contains k items
- **Support count ($\sigma$)** : Frequency of occurrence of an itemset
    - $\sigma$({Milk, Bread, Diaper}) = 2
- **Support** : Fraction of transactions that contain an itemset
    - sup({Milk, Bread, Diaper}) = 2/5
- **Frequent Itemset** : An itemset whose support is greater than or equal to a *minsup* threshold

### Association Rule

An expression of the form *A → C*, where *A* = Antecedent and *C* = Consequent are itemset

- {Milk, Diaper} → {Beer}

### Rule Evaluation Metrics

**Support (sup)** : fraction of the *N* transaction that contains both *A* and *C*

- $sup = \frac{\sigma(Milk, Diaper, Beer)}{N} = \frac25 = 0.4$

**Confidence (conf)** : measures how often all the items in *C* appear in transaction that contain *A* 

- $conf = \frac{\sigma(Milk, Diaper, Beer)}{\sigma(Milk, Diaper)} = \frac23 = 0.66$

Rules with low support can be generated by random associations.

Rules with low confidence are not really reliable.

Nevertheless a rule with relatively low support but high confidence can represent an uncommon but interesting phenomenon.

# Mining Association Rules

Given a set of transactions *N*, the goal of association rule mining is to find all rules having:

- support ≥ *minsup* threshold
- confidence ≥ *minconf* threshold

**Brute-force approach:**

- List all possible association rules
- Compute the support and confidence for each rule
- Prune rules that fail the thresholds

⇒ ***Computationally prohibitive!***

It can be observed that rules originating from the same itemset have identical support but can have different confidence ⇒ decouple the support and confidence requirement.

### Two step approach:

1. Frequent Itemset Generation
    - Generate all itemsets whose support is greater than the threshold
2. Rule Generation
    - Generate high confidence rules from each frequent itemset, where each rule is a binary partitioning of a frequent itemset

## Frequent Itemset Generation

Given *D* unique items, there are $M = 2^D$ possible candidate itemset. The complexity of the brute force is $O(NWM)$, where *W* is the medium width of the transactions. The total number of possible association rules is $R = 3^D - 2^{D+1} +1$. To reduce it we can:

- Reduce the **number of candidates** *M* with pruning techniques
- Reduce the number of comparison *NM* using efficient data structures to store the candidates and the transactions, without needing to match every candidate against every transaction.

### Reducing the number of candidates: **A priori principle**

Uses the *prior knowledge* acquired from the previous levels to reduce the search space. ****If an itemset is frequent, then all of its subsets must also be frequent. The support of an itemset never exceed the support of its subsets (**anti-monotone** property of support)

$$\forall{X,Y}:(X \sube Y) \Rightarrow sup(x) \geq sup(Y)$$

So, if an Itemset is found to be infrequent, all the itemset that contains that itemset are not checked.

- $C_k$ : candidate itemset of size *k*
- $L_k$ : frequent itemset of size *k*, represented as a table with *k* columns where each row is a frequent itemset, where the items are in lexicographic order.
- $subset_k(c)$ : set of the subsets of *c* with *k* elements

**Join Step** : $C_{k+1}$ is generated by a self join of $L_k$

```python
insert into C_k+1
select p.item_1, p.item_2, ... , p.item_k, q.item_k
from L_k p, L_k q
# Joint condition on the k-1 points
where p.item_1=q.item_1 and ... and p.item_k=q.item_k
			and p.item_k < q.item_k;
```

**Prune step :** Each (k+1)-itemset which includes a k-itemset which is not in $L_k$ is deleted from $C_{k+1}$

```python
for all c in C_k do
	for all s in subset_k-1(c) do
		if s not in L_k-1 then
			delete c from C_k
return C_k
```

$L_1 \leftarrow$ frequent 1-itemset

$k \leftarrow 1$

**while** $L_k \neq \emptyset$ **do**

$C_{k+1} =$  candidates generated from $L_k$

**for all** *t* transaction in database **do**

increment candidate count in $C_{k+1}$ for candidates found in *t*

$L_{k+1} \leftarrow \{c\} \in C_{k+1} : sup(c) \geq minsup$

$k \leftarrow k + 1$

**return** $k, L_k$ 

Even after that the number of frequent itemset can be *very large*, so it's useful to identify a small representative of frequent itemset:

- **Maximal frequent itemsets** - the smallest set of itemsets from which the frequent itemsets can be deriver
    - their immediate supersets are not frequent
    - The MFI are near the border dividing frequent by not frequent itemsets in the lattice
- **Closed itemsets** - minimal representation of itemsets without losing support information
    - an itemset is closed if none of its immediate superset has the same support
    - $X \rightarrow Y$ is redundant if there exists $X' \rightarrow Y'$ s.t. the support and the confidence are the same, $X \sube X', Y \sube Y'$

### FP-growth

It transform the problem of finding long frequent patterns into looking for shorter ones and then concatenating the suffix. It uses a compressed representation of the database using a FP-tree, that is constructed and then used recursively to mine the frequent itemsets.

- scan the database to find the support of 1-itemset
- create the root for the FP-tree
- scan the database and for each transaction
    - reorder the items for descending support
    - focus on the root node of the FP-tree
    - for each items in the transaction
        - if it matches one of the descendants of the current node, move to it and increment the count, else create a new descendant node with count 1

## Rule Generation in A priori

The confidence of a rule can be computed from the supports ⇒ for confidence based pruning of rules it is sufficient to know the support of frequent itemsets

$$conf(A \Rightarrow C) = \frac{sup(A \Rightarrow C)}{sup(A)}$$

Given a frequent itemset $L$

- find all the non-empty subsets $f \in L$ such that the confidence of rule $f \Rightarrow (L - f)$ is not less than the minimum confidence (set by the designer)
- if $|L| = k$ then there are $2^k - 2$ candidate rules ($L \Rightarrow \empty, \empty \Rightarrow L$ can be ignored)

Confidence of rules generated by the same itemset is anti-monotone w.r.t. the number of items in the RHS of the rule (decreases when we move an item from the left hand to the right hand)

$i = \{A, B,C,D\}\in L: conf(ABC \rightarrow D) \geq conf(AB \rightarrow CD) \geq conf(A \rightarrow BCD)$

- Candidate rules is generated by merging two rules that share the same prefix in the rule consequent
- join (*CD ⇒ AB*, *BD ⇒ AC*) would produce the candidate rule *D ⇒ ABC*
- Prune rule *D ⇒ ABC* if its subset *AD ⇒ BC* does not have high confidence.

## Effects of support distribution

Many real data sets have skewed support distribution. 

- if *minsup* is set too high, we could miss itemset involving interesting rare items (e.g. expensive products)
- if *minsup* is set too low it becomes computationally expensive

### Multiple Minimum Support

Define a minimum support for each item $M(i)$

The minimum support for an itemset will be $M(i, j) = \min(M(i), M(j))$

Now support is no longer anti-monotone, so the pruning method seen before does no longer holds, but FP-growth is still viable

# Pattern Evaluation

Association rules algorithms ten to produce too many rules, many of them are uninteresting or redundant (e.g. redundant if {A,B,C} → {D} and {A,B} → {D} have the same support and confidence)

Interestingness measures can be used to prune or rank the derived patterns.

Given a rule *A ⇒ C* the information needed to compute rule interestingness can be obtained from a contingency table.

For example:

$conf(Tea ⇒ Coffee) = \frac{sup(Tea, Coffee)}{sup(Tea)} = \frac{15}{20} = 0.75$

$\Pr(Coffee) = 0.9,\\ \Pr(Coffee|\bar{Tea}) = \frac{75}{80} = 0.9375$

Despite the high confidence of *Tea ⇒ Coffee,* the absence of *Tea* increases the probability of *Coffee*, so for this rule the confidence is misleading 

![Association%20Rules%2036d8e18230e44e5e8c95178312fd2fd9/Untitled%201.png](Association%20Rules%2036d8e18230e44e5e8c95178312fd2fd9/Untitled%201.png)

![Association%20Rules%2036d8e18230e44e5e8c95178312fd2fd9/Untitled%202.png](Association%20Rules%2036d8e18230e44e5e8c95178312fd2fd9/Untitled%202.png)

**Statistical Independence**

- $\Pr(A \land C) = \Pr(A) * \Pr(B)$ ⇒ **Statistical independence**
- $\Pr(A \land C) > \Pr(A) * \Pr(B)$ ⇒ **Positively correlated**
- $\Pr(A \land C) > \Pr(A) * \Pr(B)$ ⇒ **Negatively correlated**

### Lift, Leverage, Conviction

**lift** evaluates to 1 for independence. It is the ratio of true cases w.r.t. independence. Insensitive to rule direction.

$$lift(A \Rightarrow C) = \frac{conf(A \Rightarrow C)}{sup(C)} = \frac{\Pr(A, C)}{\Pr(A)\Pr(C)}$$

**leverage** evaluates to 0 for independence. It is the number of additional cases w.r.t. independence. Insensitive to rule direction.

$$leve(A \Rightarrow C) = sup(A \Rightarrow C)- sup(A)sup(C) = \Pr(A, C) - \Pr(A)\Pr(C)$$

**conviction** (a.k.a. **novelty**) is infinite if the rule is always true. It's the ratio of the expected frequency that *A* occurs without *C* (the frequency that the rule makes an incorrect prediction) if *A* and *C* were independent divided by the observed frequency of incorrect predictions. Sensitive to rule direction.

$$conv(A \Rightarrow C) = \frac{1 - sup(C)}{1 - conf(A \Rightarrow C)} = \frac{\Pr(A)(1 - \Pr(C))}{\Pr(A) - \Pr(A, C)}$$

### Intuitions

higher support 

higher confidence 

higher lift 

higher conviction 

⇒

⇒

⇒

⇒

rule applies to more records

chance that the rule is true for some record is higher

chance that the rule is just a coincidence is lower

the rule is violated less often than it would be if the antecedent and the consequent were independent

An high confidence rule can have small lift if both sides are very frequent

A low confidence rule can have high lift if both sides are very infrequent

## Properties of good measures

- $*M(A, B) = 0$* (or $1$) if $*A*$ and $*B*$ are statistically independent
- $*M(A, B)*$ increases monotonically with $\Pr(A, B)$ when $\Pr(A)$ and $\Pr(B)$ remain unchanged
- $*M(A, B)*$ decreases monotonically with $\Pr(A)$ (or $\Pr(B)$) when $\Pr(A, B)$  and $\Pr(B)$ (or $\Pr(A)$) remain unchanged

# Multidimensional Association Rules

Possible association rules between values of attributes.

E.g.: nationality=French ⇒ income=high

- Mono-dimensional (intra-attribute)
    - event: **transactions**
    - description: items A, B, and C are together in a transaction
- Multi-dimensional (inter-attribute)
    - event: **tuple**
    - description: attribute A has value a, attribute B has value b and attribute C has value c in a tuple

The two are equivalent. A multidimensional tuple with scheme (A, B, C) and values (a, b, c) can be seen as a transaction {A/a, B/b, C/c}, whereas a transaction {a, b} can be seen as a tuple with scheme (a?, b?, c?, d?) and values (yes, yes, no, no)

### Quantitative attributes

Need to discretize if there are too many distinct values for the multi/mono transformation, possibly with *equifrequency* or *mono-dimensional clustering*, for optimal covering of the original value domains

# Multilevel Association Rules

A real MBA database can include lots of distinct items, so it is necessary to find a tradeoff between general and detailed reasoning

A common **background knowledge** is the organization of the items into a hierarchy of concepts