# Evaluation of a Classifier

Evaluate how much the **theory fits the data** and the **cost generated by prediction errors.**

The evaluation is independent from the algorithm used to generate the model.

# Confidence interval in error estimation

Forecasting each element of the test set is like one experiment of a **Bernoulli process**

- good prediction ‚áí success
- bad prediction ‚áí error

If *N* is the number of independent binary random events, and *S* is the number of successes ‚áí the empirical frequency of error is *f = S/N*

This empirical frequency has a normal distribution around the true probability (for *N* ‚â• 30)

We choose the **confidence level $\alpha$,** i.e. the probability that the true frequency of success is below the pessimistic frequency that we will compute

$$P(z_{\alpha/2} \leq \frac{f - p}{\sqrt{p(1-p)/N}} \leq z_{1 - \alpha/2}) = 1 - \alpha$$

Increasing *N,* with constant empirical frequency, the uncertainty for *p* narrows

## Statistical pruning of a DT with error estimation

- consider a subtree near the leaves
- compute its maximum error $e_l$ as weighted sum of the maximum error of the leaves
- Compute the maximum error $e_r$ of the root of the subtree transformed into leaf
- prune if $e_r \leq e_l$
- With pruning, the error frequency increases, but the number of records in node also increases, therefore the maximum error can decrease

# The Hyperparameters

Are the parameter that influence the behavior of a machine learning algorithm.

Several Train/Test loops are in general necessary to find the best set of values for the hyperparameters, and for that is crucial to obtain a highly reliable estimate of the run-time performance

# Holdout

- Splitting data into *training set* and *test set*
- Splitting data into *training set, validation set* and *test set*

The split should be as random as possible, often to prevent the alteration of proportion of classes in the two set, a sampling technique called **stratification** is used

In this setting the test set is used to obtain an estimation of the performance measures with new data

# Cross Validation (k-fold)

The training set is randomly partitioned into *k* subset, stratified if necessary. Then performs *k* iteration using one of the subset for test and the others for training, and combine the result of tests

The final model is generated using the *entire training set*

- üôÅ The train/test loop is repeated *k* times
- üôÇ The estimate of the performance is averaged on *k* runs ‚áí more reliability
- üôÇ All the examples are used once for testing
- üôÇ The final model is obtained using all the example ‚áí best use of the examples

# Binary predictions

- Success rate = Accuracy $\frac{TP + TN}{N_{test}}$
- error rate = 1 - success rate

# Accuracy and other measures of a classifier

- **Precision** : $\frac{TP}{TP + FP}$
    - the rate of true positive among the positive classifications
- **Recall** : $\frac{TP}{TP + FN}$
    - the rate of the positives that I can catch (a.k.a. **Sensitivity***)*
- **Specificity** : $\frac{TN}{TN + FP}$
    - the rate of the negatives that I can catch
- **Accuracy** : $acc = sens\frac{pos}{N} + spec \frac{neg}{N}$
    - the weighted sum of sensitivity and specificity
- **F-measure** : $F = 2\frac{prec \cdot rec}{prec + rec}$
    - the harmonic mean of precision and recall (a.k.a. **F1 score** or **balanced F-score**)

## **Confusion Matrix**

- each cell contains the number of test records of class *i* and predicted as class *j*
- the numbers in the main diagonal are the "true" predictions

**True** number of $x$ labels in the dataset: $T_x = TP_x + \sum_j FP_{x - j}$ 

Total number of $x$ **predictions** by a given classifier, say $\bar{C}$: $P_x = TP_x + \sum_j FP_{j - x}$

Number of **true predictions** for class $x$ given by classifier $\bar{C}$: $TP_x$

Number of **false predictions** for class $i$ predicted as $j$: $FP_{i-j}$

Number of elements: $N = \sum_i T_i = \sum_j P_j$

- Accuracy = $\frac{\sum_j TP_j}{N}$
- Precision$_i$ = $\frac{TP_i}{P_i}$
- Recall$_i$ = $\frac{TP_i}{T_i}$

# Taking into account the random component

## $k$ statistic

Evaluates the concordance between two classification, for example between a classifier $c$ and a random assignment $r$

- Probability of concordance: $Pr(c) = \frac{\sum_i TP_i}{N}$
- Probability of random concordance: $Pr(r) = \frac{\sum_i T_i * P_i}{N^2}$

$k$ is the ration between the concordance exceeding the random component and the maximum surplus possible

$$-1 \leq k = \frac{Pr(c) - Pr(r)}{1 - Pr(r)} \leq 1$$

- 1 for perfect agreement: $\sum_i TP_i = N$
- -1 for total disagreement: $\sum_i TP_i = 0$
- 0 for random agreement

# The cost of errors

To compare two classification models and take into account the different cost of the different errors we use a ***Cost Matrix***

The correct classification do not imply any cost

Two alternatives:

- Alterate the proportion of classes in the supervised data, duplicating the examples for which the classification error is higher, in this way the classifier will become more able to classify those classes.
- Some learning scheme allow to add weights to the instances.

# Evaluation of a Probabilistic Classifier

Probabilities can be converted to a crisp value with different tecniques:

- *binary* - set a **threshold** for the positive class
- *multiclass* - output the class with the **maximum probability**

## Binary - Lift Chart

Used to evaluate various scenarios. Consider a dataset with *N* elements which *P* are positives, apply a probabilistic classification scheme and sort all the classified elements for decreasing probability of positive class.

Make a chart with axes: x - sample size, y - number of positives in sample.

The Line from (0, 0) to (100%, *P*) is that of the random choice

The area between the curve and this line is the improvement of the classifier (if this curve is above the line)

The perfect classifier shows a line from (0, 0) to (*P/N, P*) and then an horizontal line until (100%, *P*)

## ROC curve

Used to evaluate the threshold used to convert a probabilistic binary classifier into a crisp one.

The data, sorted by the probability of being positive, have gaussian distribution due to noise, then the two gaussian curves of the Positive and Negatives could be overlapped.

A threshold separate the areas into True Negatives and False Negatives, the section respectively of the Negative and the Positives below the threshold, and into False Positives and True Positives.

We can plot a point in a chart P(FP) - P(TP) for each value of the threshold. The better is the classifier, the larger is the area between this curve and the line of random assignment.

A good classifier, when t decreases, show a major increase in the TP than that of the FP

The perfect curve shows a step because all the true positives have the maximum probability, whereas the True negative have 0 probability

# From binary classifier to multi-class classification

Two way of deal with it

- Transform the training algorithm and the model, at the expense of an increased size of the problem
- Use a set of binary classifier and combine the results, at the expense of an increased number of problems to solve
    - **one-vs-one** and **one-vs-all** strategies

## One-vs-one (OVO)

Consider all the possible pairs of classes and generate a binary classifier for each pair

- N# of pairs (so classifiers) = *C (C - 1) / 2*

Each binary problem consider only the examples of the two selected classes

At prediction time apply a **voting** scheme

- an unseen example is submitted to all the binary classifiers and each winning class receives a +1
- The class with the highest number of votes wins

## One-vs-All (OVA)

Consider *C* binary problems where class *c* is a positive example, and all the others are negatives

- Build *C* binary classifiers

At prediction time apply a **voting** scheme

- an unseen example is submitted to the *C* binary classifiers obtaining a confidence score
- the confidences are combined and the class with the highest global score is chosen

## OVO vs OVA

OVO requires solving an higher number of problems, even if they are of smaller size

OVA tends to be intrinsically unbalanced, because if the classes are evenly distributed in the examples, each classifier has a proportion positive to negative 1 to *C* - 1

# Ensemble methods

- Train a set of **base classifiers**
- The final prediction is obtained taking the votes of the base classifiers
- ensemble methods tend to perform better than a single classifier, if they are independent from each other, because the ensemble will be wrong only if the majority of the base classifiers is wrong
- But if the base classifiers have an error rate ‚â• 0.5, the ensemble gets worse results!

So the ensemble methods are useful if:

- the base classifiers are independent
- the performance of the base classifier is better than the random choice

## Methods for ensemble classifiers

### By manipulating the training set

Data are resampled according to some sampling strategy

- **Bagging**: repeatedly samples with replacement according to a uniform probability distribution
- **Boosting**: iteratively changes the distribution of training examples so that the base classifier focus on examples which are hard to classify
- **Adaboost**: the importance of each base classifier depends on its error rate

### By manipulating the input features

Subset of input features, split according to the correlation between them and the goal, or randomly, or according to suggestion from domain experts

- **Random forest**: uses decision trees as base classifiers

### By manipulating the class labels

Useful when the number of classes is high. For each base classifier, randomly partition class labels into two subsets, say $A_1, A_2$ and re-label the dataset.

Train binary classifiers with respect to the two classes

At testing time, when a subset is selected all the classes it includes receive a vote, and the class with the top score wins.